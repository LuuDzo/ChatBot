{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oa_VO6HvZu22",
        "outputId": "28b7fe84-0586-4246-ee59-2fc87de315cd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import nltk\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils import to_categorical\n",
        "nltk.download('punkt')  # Cần tải dữ liệu cho tokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import regex as re\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Punkt tokenizer is available.\n",
            "['C:\\\\Users\\\\ASUS/nltk_data', 'c:\\\\Users\\\\ASUS\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data', 'c:\\\\Users\\\\ASUS\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data', 'c:\\\\Users\\\\ASUS\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\ASUS\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n"
          ]
        }
      ],
      "source": [
        "import nltk.data\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    print(\"Punkt tokenizer is available.\")\n",
        "except LookupError:\n",
        "    print(\"Punkt tokenizer is not available. Downloading it...\")\n",
        "    nltk.download('punkt')\n",
        "print(nltk.data.path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "O4fgy0tnuuWw"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'intents': [{'tag': 'greeting',\n",
              "   'patterns': ['Xin chào ad',\n",
              "    'Chào admin',\n",
              "    'Ad ơi',\n",
              "    'hi',\n",
              "    'hello',\n",
              "    'có ai ở đây không ạ',\n",
              "    'bắt đầu',\n",
              "    'Hey!',\n",
              "    'ad ơi em muốn tư vấn tour với ạ',\n",
              "    'ai có thể giúp em không',\n",
              "    'dạ cho em hỏi',\n",
              "    'cho hỏi có ai chat được bây giờ không',\n",
              "    'em cần tư vấn ạ',\n",
              "    'làm ơn giúp em',\n",
              "    'em đang cần một ít thông tin',\n",
              "    'chào ad, em có một câu hỏi',\n",
              "    'em có một vấn đề cần được giải thích'],\n",
              "   'responses': ['Chào bạn, DANANG tourist chuyên về các tour của thành phố Đà Nẵng xin nghe.Mình có thể giúp gì cho bạn được ạ']},\n",
              "  {'tag': 'goodbye',\n",
              "   'patterns': ['Bye',\n",
              "    'goodbye',\n",
              "    'good bye',\n",
              "    'Tạm biệt',\n",
              "    'Bai',\n",
              "    'pp',\n",
              "    'hẹn gặp lại',\n",
              "    'bái bai ad nha',\n",
              "    'cảm ơn và chào tạm biệt',\n",
              "    'chúc một ngày tốt lành và tạm biệt',\n",
              "    'em sẽ liên hệ lại sau ạ',\n",
              "    'em sẽ quay lại sau',\n",
              "    'em sẽ tìm đến ad sau',\n",
              "    'em sẽ quay lại nếu có thêm câu hỏi'],\n",
              "   'responses': ['Chúc bạn ngày mới tốt đẹp',\n",
              "    'Hẹn bạn một ngày tuyệt vời tại DANANG tourist ạ',\n",
              "    'Hy vọng bạn đã có câu trả lời tốt nhất cho mình , hẹn bạn một người đẹp trời tại DANANG tourist']},\n",
              "  {'tag': 'thanks',\n",
              "   'patterns': ['Cảm ơn ',\n",
              "    'thanks',\n",
              "    'tks',\n",
              "    'thank you',\n",
              "    'tks ad',\n",
              "    'cảm ơn đã giúp đỡ em',\n",
              "    'cảm ơn vì đã tư vấn ạ',\n",
              "    'em xin cảm ơn',\n",
              "    'cảm ơn bên mình vì nhiệt tình ạ'],\n",
              "   'responses': ['cảm ơn vì bạn đã tin tưởng DANANG tourist ạ',\n",
              "    'mình rất vui vì đã giúp được bạn',\n",
              "    'mình rất hân hịnh khi được giúp bạn ạ',\n",
              "    'cảm ơn vì đã tin tưởng DANANG tourist ạ']},\n",
              "  {'tag': 'not_understand',\n",
              "   'patterns': ['mình có được mua đồ không ',\n",
              "    'thời gian đi tốn nhiều không',\n",
              "    'điểm khác nhau giữa các hành khách trong đoàn',\n",
              "    'không muốn đi có bị sao không',\n",
              "    'làm sao để không được đi',\n",
              "    'muốn đề nghị đổi hành khách chung tour',\n",
              "    'tôi có được ưu tiên gì không',\n",
              "    'đi không cần đóng tiền được không',\n",
              "    'biết tôi là ai không',\n",
              "    'bạn có món gì ngon',\n",
              "    'ngày mai sẽ có mưa không',\n",
              "    'tôi muốn được đi ngủ',\n",
              "    'tôi không thích đi xe '],\n",
              "   'responses': ['Xin lỗi, mình không thể hiểu được bạn đang nói gì . Bạn có thể giải thích chi tiết hơn được không ?',\n",
              "    'Mình không hiểu ý bạn lắm',\n",
              "    'xin lỗi nhưng bạn có thể giải thích chi tiết hơn không',\n",
              "    'bạn ơi , mình không hiểu ý của bạn lắm , mình mong bạn giải thích chi tiết hơn á ',\n",
              "    'Bạn có thể trình bày kĩ hơn những thắc mắc của bạn không ? mình xin lỗi vì bất tiện này']},\n",
              "  {'tag': 'HuongDanDangKi',\n",
              "   'patterns': ['Làm thế nào để đăng ký được tour?',\n",
              "    'tôi muốn đăng kí tour thì phải làm như nào',\n",
              "    'Hướng dẫn cách đăng kí tour',\n",
              "    'Em muốn đăng kí tour thì phải làm những gì ạ',\n",
              "    'Cần làm gì để đăng kí tour ạ',\n",
              "    'bên mình còn cho đăng kí tour không ạ',\n",
              "    'Em cần tư vấn về đăng kí tour với ạ',\n",
              "    'Đăng kí tour làm như nào',\n",
              "    'phải làm gì để đăng kí tour'],\n",
              "   'responses': ['bạn có thể đăng kí tour bên mình ,bạn có liên hệ với số điện thoại tổng đài 0858.752.736 bên mình tư vấn ạ,hoặc để lại thông tin số điện thoại ,để nhân viên bên mình tư vấn ạ. ']},\n",
              "  {'tag': 'end_of_tour',\n",
              "   'patterns': ['hạn cuối đăng kí tour là khi nào?',\n",
              "    'cần thông tin về hạn cuối đăng kí tour',\n",
              "    'Làm thế nào để biết hạn cuối đăng kí tour?',\n",
              "    'ad cho e hỏi ngày cuối để đăng kí tour là khi nào',\n",
              "    'Em cần biết về hạn cuối đăng kí tour ạ',\n",
              "    'Ngày mấy là hạn cuối đăng kí ad'],\n",
              "   'responses': ['Hạn cuối đăng kí các tour sẽ là ngày 6 của tháng ạ. Đừng quên đăng kí sớm để có trải nghiệm vui chơi tốt nhất ạ.']},\n",
              "  {'tag': 'Register',\n",
              "   'patterns': ['tôi muốn đăng kí tour bên bạn?',\n",
              "    'bên bạn có những tour nào',\n",
              "    'có tour nào bên bạn không?',\n",
              "    'gợi ý tour bên bạn',\n",
              "    'loại tour bên bạn có'],\n",
              "   'responses': ['A1,A2,A3(2 ngày 1 đêm),B1,B2,B3(3 ngày 2 đêm),C1,C2,C3(4 ngày 3 đêm),D1(5 ngày 4 đêm)']},\n",
              "  {'tag': 'majors_offered',\n",
              "   'patterns': ['Các trải nghiệm tôi có thể có với tour của bạn?',\n",
              "    'Tôi muốn biết về các trải nghiệm tôi sẽ có được',\n",
              "    'Có những trải nghiệm gì?',\n",
              "    'Ad có thể giới thiệu cho em về vài nổi bật của tour mình với ạ',\n",
              "    'Em muốn tìm hiểu về các trải nghiệm của tour mình ạ',\n",
              "    'Các trải nghiệm của tour mình?',\n",
              "    'bên mình có các loại trải nghiệm gì ạ?'],\n",
              "   'responses': ['Tour du lịch ở Đà Nẵng có thể bao gồm thăm các điểm du lịch nổi tiếng như Bà Nà Hills, Ngũ Hành Sơn, Cầu Rồng, và khu vực biển Mỹ Khê. Ngoài ra, bạn cũng có thể khám phá ẩm thực địa phương, tham gia các hoạt động giải trí, và thư giãn tại các resort ven biển.']},\n",
              "  {'tag': 'accommodation',\n",
              "   'patterns': ['tour đã bao gồm nơi nghỉ ngơi chưa ạ?',\n",
              "    'Thông tin về nơi nghỉ ngơi',\n",
              "    'Làm thế nào để chọn được nơi nghỉ ngơi?',\n",
              "    'tour của mình đã bao gồm cả nơi nghỉ rồi đúng không ad?',\n",
              "    'nơi nghỉ ngơi.',\n",
              "    'Chi phí của nơi nghỉ ngơi.',\n",
              "    'giá của nơi nghỉ ngơi được bao gồm trong tour rồi chứ ạ?',\n",
              "    'Địa chỉ nơi nghỉ ngơi ở đâu vậy ad?',\n",
              "    'Mức giá của nơi nghỉ ngơi.',\n",
              "    'Nơi nghỉ ngơi có hiện đại không ạ?'],\n",
              "   'responses': ['Nơi nghỉ ngơi có nhiều loại phòng khác nhau ạ. Đối với {} thì giá qua đêm sẽ là {} đã được thanh toán khi đăng kí tour ạ.']},\n",
              "  {'tag': 'outside_events',\n",
              "   'patterns': ['tour có các hoạt động ngoài trời không ạ',\n",
              "    'Có sự kiện ngoài trời nào không?',\n",
              "    'Hoạt động ngoài trời khi nào vậy ad?',\n",
              "    'Hoạt động ngoài trời có gì hấp dẫn không ad?',\n",
              "    'tổ chức các sự kiện trong khi đi tour được không ạ ?.',\n",
              "    'Làm thế nào để tham gia hoạt động ngoài trời khi đi tour ạ?',\n",
              "    'Tôi muốn tổ chức sự kiện ngoài trời',\n",
              "    'tôi muốn tổ chức sinh nhật trong tour',\n",
              "    'tôi muốn tổ chức đám cưới trong tour'],\n",
              "   'responses': ['Tour thường tổ chức các sự kiện như hoạt động ngoài trời nếu bạn có nhu cầu tham gia.']},\n",
              "  {'tag': 'courses',\n",
              "   'patterns': ['Có những tour nào được cung cấp bên bạn ?',\n",
              "    'Danh sách các tour bên bạn',\n",
              "    'Tôi muốn biết về các tour bên bạn.'],\n",
              "   'responses': ['DANAN tourist cung cấp hàng loạt tour và đa dạng thời gian cho bạn lựa chọn.Hiện nay có các tour A1,A2,A3,B1,B2,B3,C1,C2,C3,D1']},\n",
              "  {'tag': 'AnSang',\n",
              "   'patterns': ['các món ăn được phục vụ khi đi tour?',\n",
              "    'đồ ăn của tour',\n",
              "    'Tôi muốn tìm hiểu về ẩm thực của tour.',\n",
              "    'Cho mình hỏi, tour bên mình có bao gồm đồ ăn chưa ạ',\n",
              "    'mình muốn trải nghiệm ẩm thực địa phương thì được không ad',\n",
              "    'các loại hình ẩm thực của tour'],\n",
              "   'responses': ['Tour đã bao gồm dịch vụ ăn uống cho quý khách, Chúng ta sẽ trải nghiệm ẩm thực địa phương vào buổi sáng, còn các buổi còn lại quý khách sẽ được phục vụ ăn uống tại nhà hàng nổi tiếng của địa phương.']},\n",
              "  {'tag': 'mountain',\n",
              "   'patterns': ['Các địa điểm ngắm cảnh đẹp của Đà Nẵng?',\n",
              "    'Nơi ngắm cảnh đẹp của thành phố',\n",
              "    'Tôi muốn hỏi về những nơi ngắm cảnh đẹp.',\n",
              "    'Tour có đi những nơi ngắm cảnh đẹp không',\n",
              "    'Tôi muốn leo núi,tour có địa điểm leo núi nào không'],\n",
              "   'responses': ['Tour sẽ tổ chức cho bạn ngắm cảnh ở các địa điểm có cảnh đẹp như :Sơn Trà,Đỉnh Bàn cờ , Bãi đá đen,...']},\n",
              "  {'tag': 'beach',\n",
              "   'patterns': ['Các bãi biển đẹp của Đà Nẵng?',\n",
              "    'Nơi ngắm cảnh biển đẹp của thành phố',\n",
              "    'Tôi muốn hỏi về những bãi biển của thành phố.',\n",
              "    'Tour có đi các bãi biển không',\n",
              "    'Những bãi biển đẹp Đà Nẵng',\n",
              "    'Có tổ chức đi tham quan những bãi biển không?',\n",
              "    'Có tổ chức đi biển không'],\n",
              "   'responses': ['Tour sẽ tổ chức cho bạn tham quan các bãi biển đẹp của Đà Nẵng như : biển mỹ khê,biển non nước,bãi rạn,...']},\n",
              "  {'tag': 'camping',\n",
              "   'patterns': ['Có dịch vụ cắm trại không?',\n",
              "    'cắm trại khi đi tour',\n",
              "    'Tôi muốn biết về cắm trại trong tour được không ?',\n",
              "    'cho em hỏi đang đi tour thì mình tổ chức cắm trại được không ạ',\n",
              "    'Em muốn được tổ chức cắm trại ',\n",
              "    'em muốn hỏi tour mình có tổ chức cắm trại không ạ'],\n",
              "   'responses': ['Tour sẽ có tổ chức cắm trại ở duy nhất tour D1.']},\n",
              "  {'tag': 'swim',\n",
              "   'patterns': ['Được phép tắm biển không ?',\n",
              "    'Có được đi tắm biển không ?',\n",
              "    'Tôi muốn tham gia tắm biển.',\n",
              "    'Em muốn hỏi là tour mình được tắm  biển không?',\n",
              "    'em biết bơi thì có được đi tắm biển không ?',\n",
              "    'có tắm biển được không ?',\n",
              "    'tour có đi tắm biển không ?'],\n",
              "   'responses': ['Tất cả các tour đều có hoạt động tăm biển ạ.']},\n",
              "  {'tag': 'location',\n",
              "   'patterns': ['Có tour nào tham quan địa điểm không ?',\n",
              "    'tôi muốn tham quan điểm du lịch thì đi tour nào',\n",
              "    'những tour tham quan nào có địa điểm',\n",
              "    'địa điểm thì đi tour nào'],\n",
              "   'responses': ['Về điểm du lịch {},bên em có các tour {}. ']},\n",
              "  {'tag': 'location_information',\n",
              "   'patterns': ['tôi muốn tìm hiểu thông tin về địa điểm',\n",
              "    'chi tiết về địa điểm ',\n",
              "    'Cho tôi thông tin về địa điểm',\n",
              "    'địa điểm gồm những gì'],\n",
              "   'responses': ['Thông tin của {}:{}']},\n",
              "  {'tag': 'price',\n",
              "   'patterns': ['cho em hỏi chi phí với ạ',\n",
              "    'Dạ ad cho em hỏi chi phí với ạ',\n",
              "    'Dạ ad cho em hỏi hết bao nhiêu với ạ',\n",
              "    'Ad cho em hỏi chi phí là như thế nào ạ',\n",
              "    'Tiền bao nhiêu',\n",
              "    'Chi phí phải trả của ',\n",
              "    'ad cho hỏi chi phí bao nhiêu tiền ',\n",
              "    'Tiền là bao nhiêu ',\n",
              "    'chi phí là mấy',\n",
              "    'Cho em hỏi chi phí là mấy',\n",
              "    'Dạ ad cho hỏi chi phí với',\n",
              "    'Ad cho em hỏi giá là bao tiền ạ',\n",
              "    'Tiền mấy tiền vậy',\n",
              "    'Chi phí là bao nhiêu thế',\n",
              "    'Dạ ad cho hỏi mấy tiền vậy ạ',\n",
              "    'Tổng tiền là mấy'],\n",
              "   'responses': ['giá thành cho {} là {} triệu đồng/1 người bạn nhé!']},\n",
              "  {'tag': 'LichTrinh',\n",
              "   'patterns': ['cho em hỏi lịch trình ạ',\n",
              "    'Lịch trình bao gồm những điểm tham quan nào?',\n",
              "    'bao gồm những điểm đến nào?'],\n",
              "   'responses': ['Lịch trình của {} gồm những địa điểm sau {}']},\n",
              "  {'tag': 'Cancel_Tour',\n",
              "   'patterns': ['Nếu hủy tôi sẽ được hoàn trả bao nhiêu tiền?',\n",
              "    'Nếu tôi hủy và chuyển đổi tour khác thì có được hoàn trả tiền tour không?',\n",
              "    'Hủy tour thì được trả bao nhiêu tiền',\n",
              "    'Tôi được trả lại bao nhiêu tiền nếu hủy '],\n",
              "   'responses': ['Việc hủy Tour sẽ tùy vào thời điểm và tất cả trường hợp người cao tuổi , trẻ em đều hoàn lại 1 giá cố định ,đối với {} thì sẽ hoàn lại {}/1 người ạ']},\n",
              "  {'tag': 'Cut_price',\n",
              "   'patterns': ['Bên bạn có chương trình giảm giá không',\n",
              "    'Chương trình giảm giá bên bạn áp dụng cho đối tượng'],\n",
              "   'responses': ['Việc giảm giá sẽ tùy thời điểm,Đối với {} thì giá sau khi giảm sẽ còn là {}']},\n",
              "  {'tag': 'pet',\n",
              "   'patterns': ['có được mang theo thú cưng khi đi tour không ?',\n",
              "    'tour có cho phép mang theo thú cưng không ?',\n",
              "    'có được mang theo chó không ?',\n",
              "    'có được mang theo mèo không ?',\n",
              "    'mang theo chó, mèo có bị sao không?'],\n",
              "   'responses': ['Tour có cho phép mang theo thú cưng ạ']},\n",
              "  {'tag': 'moving',\n",
              "   'patterns': ['tour đã bao gồm phương tiện di chuyển rồi chứ ?',\n",
              "    'phương tiện di chuyển trong tour đã được tính phí rồi chứ ? ',\n",
              "    'tour có bao gồm phương tiện di chuyển không ?',\n",
              "    'tour đã bao gồm phí di chuyển rồi chứ ?'],\n",
              "   'responses': ['Tour đã bao gồm chi phí di chuyển rồi ạ']},\n",
              "  {'tag': 'time_start',\n",
              "   'patterns': ['Thời gian bắt đầu cho chuyến đi là mấy giờ ?',\n",
              "    'Tour sẽ khởi hành lúc nào ?',\n",
              "    'Tour sẽ xuất phát lúc nào ?',\n",
              "    'Mấy giờ thì tour bắt đầu khởi hành'],\n",
              "   'responses': ['tour sẽ khởi hành trong khoảng từ 8h00-9h00 buổi sáng ngày khởi hành,quý khách vui lòng có mặt trước 8h00 để làm thủ tục và ổn định vị trí']},\n",
              "  {'tag': 'introduce',\n",
              "   'patterns': ['giới thiệu về DANANG TOURIST',\n",
              "    'tôi cần thông tin về DANANG TOURIST',\n",
              "    'DANANG TOURIST là gì',\n",
              "    'tôi muốn biết về DANANG TOURIST'],\n",
              "   'responses': ['DANANG TOURIST là công ty du lịch , chuyên về các tour du lịch của thành phố Đà Nẵng và các thành phố lân cận. Với niềm đam mê du lịch, khám phá chúng tôi đã cùng nhau xây dựng website nơi mà những nhà lữ hành có thể tìm hiểu về các tour du lịch của Đà Nẵng. Chúng tôi sẽ mong muốn sẽ cung cấp những dịch vụ tốt nhất cho quý khách <3']},\n",
              "  {'tag': 'hope',\n",
              "   'patterns': ['vì sao tôi phải chọn DANANG TOURIST',\n",
              "    'Cho tôi lý do vì sao tôi phải chọn bạn',\n",
              "    'DANANG TOURIST có gì khác so với các công ty du lịch khác ?'],\n",
              "   'responses': ['chúng tôi mong muốn du khách tận hưởng các dịch vụ tốt nhất qua sự trải nghiệm thực tế của chúng tôi , chúng tôi chắc chắn mang lại cho quý khách khoảng thời gian đáng nhớ với: đội ngũ hướng dẫn viên chuyên nghiệp và tận tâm , những địa điểm du lịch hấp dẫn , mức giá phải chăng.']}]}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open('intents.json','r',encoding='utf-8') as file:\n",
        "    intents = json.load(file)\n",
        "intents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZHty8-9PtzSV"
      },
      "outputs": [],
      "source": [
        "uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n",
        "unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
        "\n",
        "\n",
        "def loaddicchar():\n",
        "    dic = {}\n",
        "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
        "        '|')\n",
        "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
        "        '|')\n",
        "    for i in range(len(char1252)):\n",
        "        dic[char1252[i]] = charutf8[i]\n",
        "    return dic\n",
        "\n",
        "\n",
        "dicchar = loaddicchar()\n",
        "\n",
        "# Đưa toàn bộ dữ liệu qua hàm này để chuẩn hóa lại\n",
        "def convert_unicode(txt):\n",
        "    return re.sub(\n",
        "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
        "        lambda x: dicchar[x.group()], txt)\n",
        "def text_process(text):\n",
        "    text = convert_unicode(text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^swáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ_]',' ',text)\n",
        "    text = re.sub(r's+', ' ', text).strip()\n",
        "    return text\n",
        "def sen_to_vec(tokenized_sen,all_words):\n",
        "    vec = np.zeros(len(all_words),dtype=np.float32)\n",
        "    for index,w in enumerate(all_words):\n",
        "        if w in tokenized_sen:\n",
        "            vec[index] = 1.0\n",
        "    return vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "pzeDiBAoq9eJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'intents': [{'tag': 'greeting', 'patterns': ['Xin chào ad', 'Chào admin', 'Ad ơi', 'hi', 'hello', 'có ai ở đây không ạ', 'bắt đầu', 'Hey!', 'ad ơi em muốn tư vấn tour với ạ', 'ai có thể giúp em không', 'dạ cho em hỏi', 'cho hỏi có ai chat được bây giờ không', 'em cần tư vấn ạ', 'làm ơn giúp em', 'em đang cần một ít thông tin', 'chào ad, em có một câu hỏi', 'em có một vấn đề cần được giải thích'], 'responses': ['Chào bạn, DANANG tourist chuyên về các tour của thành phố Đà Nẵng xin nghe.Mình có thể giúp gì cho bạn được ạ']}, {'tag': 'goodbye', 'patterns': ['Bye', 'goodbye', 'good bye', 'Tạm biệt', 'Bai', 'pp', 'hẹn gặp lại', 'bái bai ad nha', 'cảm ơn và chào tạm biệt', 'chúc một ngày tốt lành và tạm biệt', 'em sẽ liên hệ lại sau ạ', 'em sẽ quay lại sau', 'em sẽ tìm đến ad sau', 'em sẽ quay lại nếu có thêm câu hỏi'], 'responses': ['Chúc bạn ngày mới tốt đẹp', 'Hẹn bạn một ngày tuyệt vời tại DANANG tourist ạ', 'Hy vọng bạn đã có câu trả lời tốt nhất cho mình , hẹn bạn một người đẹp trời tại DANANG tourist']}, {'tag': 'thanks', 'patterns': ['Cảm ơn ', 'thanks', 'tks', 'thank you', 'tks ad', 'cảm ơn đã giúp đỡ em', 'cảm ơn vì đã tư vấn ạ', 'em xin cảm ơn', 'cảm ơn bên mình vì nhiệt tình ạ'], 'responses': ['cảm ơn vì bạn đã tin tưởng DANANG tourist ạ', 'mình rất vui vì đã giúp được bạn', 'mình rất hân hịnh khi được giúp bạn ạ', 'cảm ơn vì đã tin tưởng DANANG tourist ạ']}, {'tag': 'not_understand', 'patterns': ['mình có được mua đồ không ', 'thời gian đi tốn nhiều không', 'điểm khác nhau giữa các hành khách trong đoàn', 'không muốn đi có bị sao không', 'làm sao để không được đi', 'muốn đề nghị đổi hành khách chung tour', 'tôi có được ưu tiên gì không', 'đi không cần đóng tiền được không', 'biết tôi là ai không', 'bạn có món gì ngon', 'ngày mai sẽ có mưa không', 'tôi muốn được đi ngủ', 'tôi không thích đi xe '], 'responses': ['Xin lỗi, mình không thể hiểu được bạn đang nói gì . Bạn có thể giải thích chi tiết hơn được không ?', 'Mình không hiểu ý bạn lắm', 'xin lỗi nhưng bạn có thể giải thích chi tiết hơn không', 'bạn ơi , mình không hiểu ý của bạn lắm , mình mong bạn giải thích chi tiết hơn á ', 'Bạn có thể trình bày kĩ hơn những thắc mắc của bạn không ? mình xin lỗi vì bất tiện này']}, {'tag': 'HuongDanDangKi', 'patterns': ['Làm thế nào để đăng ký được tour?', 'tôi muốn đăng kí tour thì phải làm như nào', 'Hướng dẫn cách đăng kí tour', 'Em muốn đăng kí tour thì phải làm những gì ạ', 'Cần làm gì để đăng kí tour ạ', 'bên mình còn cho đăng kí tour không ạ', 'Em cần tư vấn về đăng kí tour với ạ', 'Đăng kí tour làm như nào', 'phải làm gì để đăng kí tour'], 'responses': ['bạn có thể đăng kí tour bên mình ,bạn có liên hệ với số điện thoại tổng đài 0858.752.736 bên mình tư vấn ạ,hoặc để lại thông tin số điện thoại ,để nhân viên bên mình tư vấn ạ. ']}, {'tag': 'end_of_tour', 'patterns': ['hạn cuối đăng kí tour là khi nào?', 'cần thông tin về hạn cuối đăng kí tour', 'Làm thế nào để biết hạn cuối đăng kí tour?', 'ad cho e hỏi ngày cuối để đăng kí tour là khi nào', 'Em cần biết về hạn cuối đăng kí tour ạ', 'Ngày mấy là hạn cuối đăng kí ad'], 'responses': ['Hạn cuối đăng kí các tour sẽ là ngày 6 của tháng ạ. Đừng quên đăng kí sớm để có trải nghiệm vui chơi tốt nhất ạ.']}, {'tag': 'Register', 'patterns': ['tôi muốn đăng kí tour bên bạn?', 'bên bạn có những tour nào', 'có tour nào bên bạn không?', 'gợi ý tour bên bạn', 'loại tour bên bạn có'], 'responses': ['A1,A2,A3(2 ngày 1 đêm),B1,B2,B3(3 ngày 2 đêm),C1,C2,C3(4 ngày 3 đêm),D1(5 ngày 4 đêm)']}, {'tag': 'majors_offered', 'patterns': ['Các trải nghiệm tôi có thể có với tour của bạn?', 'Tôi muốn biết về các trải nghiệm tôi sẽ có được', 'Có những trải nghiệm gì?', 'Ad có thể giới thiệu cho em về vài nổi bật của tour mình với ạ', 'Em muốn tìm hiểu về các trải nghiệm của tour mình ạ', 'Các trải nghiệm của tour mình?', 'bên mình có các loại trải nghiệm gì ạ?'], 'responses': ['Tour du lịch ở Đà Nẵng có thể bao gồm thăm các điểm du lịch nổi tiếng như Bà Nà Hills, Ngũ Hành Sơn, Cầu Rồng, và khu vực biển Mỹ Khê. Ngoài ra, bạn cũng có thể khám phá ẩm thực địa phương, tham gia các hoạt động giải trí, và thư giãn tại các resort ven biển.']}, {'tag': 'accommodation', 'patterns': ['tour đã bao gồm nơi nghỉ ngơi chưa ạ?', 'Thông tin về nơi nghỉ ngơi', 'Làm thế nào để chọn được nơi nghỉ ngơi?', 'tour của mình đã bao gồm cả nơi nghỉ rồi đúng không ad?', 'nơi nghỉ ngơi.', 'Chi phí của nơi nghỉ ngơi.', 'giá của nơi nghỉ ngơi được bao gồm trong tour rồi chứ ạ?', 'Địa chỉ nơi nghỉ ngơi ở đâu vậy ad?', 'Mức giá của nơi nghỉ ngơi.', 'Nơi nghỉ ngơi có hiện đại không ạ?'], 'responses': ['Nơi nghỉ ngơi có nhiều loại phòng khác nhau ạ. Đối với {} thì giá qua đêm sẽ là {} đã được thanh toán khi đăng kí tour ạ.']}, {'tag': 'outside_events', 'patterns': ['tour có các hoạt động ngoài trời không ạ', 'Có sự kiện ngoài trời nào không?', 'Hoạt động ngoài trời khi nào vậy ad?', 'Hoạt động ngoài trời có gì hấp dẫn không ad?', 'tổ chức các sự kiện trong khi đi tour được không ạ ?.', 'Làm thế nào để tham gia hoạt động ngoài trời khi đi tour ạ?', 'Tôi muốn tổ chức sự kiện ngoài trời', 'tôi muốn tổ chức sinh nhật trong tour', 'tôi muốn tổ chức đám cưới trong tour'], 'responses': ['Tour thường tổ chức các sự kiện như hoạt động ngoài trời nếu bạn có nhu cầu tham gia.']}, {'tag': 'courses', 'patterns': ['Có những tour nào được cung cấp bên bạn ?', 'Danh sách các tour bên bạn', 'Tôi muốn biết về các tour bên bạn.'], 'responses': ['DANAN tourist cung cấp hàng loạt tour và đa dạng thời gian cho bạn lựa chọn.Hiện nay có các tour A1,A2,A3,B1,B2,B3,C1,C2,C3,D1']}, {'tag': 'AnSang', 'patterns': ['các món ăn được phục vụ khi đi tour?', 'đồ ăn của tour', 'Tôi muốn tìm hiểu về ẩm thực của tour.', 'Cho mình hỏi, tour bên mình có bao gồm đồ ăn chưa ạ', 'mình muốn trải nghiệm ẩm thực địa phương thì được không ad', 'các loại hình ẩm thực của tour'], 'responses': ['Tour đã bao gồm dịch vụ ăn uống cho quý khách, Chúng ta sẽ trải nghiệm ẩm thực địa phương vào buổi sáng, còn các buổi còn lại quý khách sẽ được phục vụ ăn uống tại nhà hàng nổi tiếng của địa phương.']}, {'tag': 'mountain', 'patterns': ['Các địa điểm ngắm cảnh đẹp của Đà Nẵng?', 'Nơi ngắm cảnh đẹp của thành phố', 'Tôi muốn hỏi về những nơi ngắm cảnh đẹp.', 'Tour có đi những nơi ngắm cảnh đẹp không', 'Tôi muốn leo núi,tour có địa điểm leo núi nào không'], 'responses': ['Tour sẽ tổ chức cho bạn ngắm cảnh ở các địa điểm có cảnh đẹp như :Sơn Trà,Đỉnh Bàn cờ , Bãi đá đen,...']}, {'tag': 'beach', 'patterns': ['Các bãi biển đẹp của Đà Nẵng?', 'Nơi ngắm cảnh biển đẹp của thành phố', 'Tôi muốn hỏi về những bãi biển của thành phố.', 'Tour có đi các bãi biển không', 'Những bãi biển đẹp Đà Nẵng', 'Có tổ chức đi tham quan những bãi biển không?', 'Có tổ chức đi biển không'], 'responses': ['Tour sẽ tổ chức cho bạn tham quan các bãi biển đẹp của Đà Nẵng như : biển mỹ khê,biển non nước,bãi rạn,...']}, {'tag': 'camping', 'patterns': ['Có dịch vụ cắm trại không?', 'cắm trại khi đi tour', 'Tôi muốn biết về cắm trại trong tour được không ?', 'cho em hỏi đang đi tour thì mình tổ chức cắm trại được không ạ', 'Em muốn được tổ chức cắm trại ', 'em muốn hỏi tour mình có tổ chức cắm trại không ạ'], 'responses': ['Tour sẽ có tổ chức cắm trại ở duy nhất tour D1.']}, {'tag': 'swim', 'patterns': ['Được phép tắm biển không ?', 'Có được đi tắm biển không ?', 'Tôi muốn tham gia tắm biển.', 'Em muốn hỏi là tour mình được tắm  biển không?', 'em biết bơi thì có được đi tắm biển không ?', 'có tắm biển được không ?', 'tour có đi tắm biển không ?'], 'responses': ['Tất cả các tour đều có hoạt động tăm biển ạ.']}, {'tag': 'location', 'patterns': ['Có tour nào tham quan địa điểm không ?', 'tôi muốn tham quan điểm du lịch thì đi tour nào', 'những tour tham quan nào có địa điểm', 'địa điểm thì đi tour nào'], 'responses': ['Về điểm du lịch {},bên em có các tour {}. ']}, {'tag': 'location_information', 'patterns': ['tôi muốn tìm hiểu thông tin về địa điểm', 'chi tiết về địa điểm ', 'Cho tôi thông tin về địa điểm', 'địa điểm gồm những gì'], 'responses': ['Thông tin của {}:{}']}, {'tag': 'price', 'patterns': ['cho em hỏi chi phí với ạ', 'Dạ ad cho em hỏi chi phí với ạ', 'Dạ ad cho em hỏi hết bao nhiêu với ạ', 'Ad cho em hỏi chi phí là như thế nào ạ', 'Tiền bao nhiêu', 'Chi phí phải trả của ', 'ad cho hỏi chi phí bao nhiêu tiền ', 'Tiền là bao nhiêu ', 'chi phí là mấy', 'Cho em hỏi chi phí là mấy', 'Dạ ad cho hỏi chi phí với', 'Ad cho em hỏi giá là bao tiền ạ', 'Tiền mấy tiền vậy', 'Chi phí là bao nhiêu thế', 'Dạ ad cho hỏi mấy tiền vậy ạ', 'Tổng tiền là mấy'], 'responses': ['giá thành cho {} là {} triệu đồng/1 người bạn nhé!']}, {'tag': 'LichTrinh', 'patterns': ['cho em hỏi lịch trình ạ', 'Lịch trình bao gồm những điểm tham quan nào?', 'bao gồm những điểm đến nào?'], 'responses': ['Lịch trình của {} gồm những địa điểm sau {}']}, {'tag': 'Cancel_Tour', 'patterns': ['Nếu hủy tôi sẽ được hoàn trả bao nhiêu tiền?', 'Nếu tôi hủy và chuyển đổi tour khác thì có được hoàn trả tiền tour không?', 'Hủy tour thì được trả bao nhiêu tiền', 'Tôi được trả lại bao nhiêu tiền nếu hủy '], 'responses': ['Việc hủy Tour sẽ tùy vào thời điểm và tất cả trường hợp người cao tuổi , trẻ em đều hoàn lại 1 giá cố định ,đối với {} thì sẽ hoàn lại {}/1 người ạ']}, {'tag': 'Cut_price', 'patterns': ['Bên bạn có chương trình giảm giá không', 'Chương trình giảm giá bên bạn áp dụng cho đối tượng'], 'responses': ['Việc giảm giá sẽ tùy thời điểm,Đối với {} thì giá sau khi giảm sẽ còn là {}']}, {'tag': 'pet', 'patterns': ['có được mang theo thú cưng khi đi tour không ?', 'tour có cho phép mang theo thú cưng không ?', 'có được mang theo chó không ?', 'có được mang theo mèo không ?', 'mang theo chó, mèo có bị sao không?'], 'responses': ['Tour có cho phép mang theo thú cưng ạ']}, {'tag': 'moving', 'patterns': ['tour đã bao gồm phương tiện di chuyển rồi chứ ?', 'phương tiện di chuyển trong tour đã được tính phí rồi chứ ? ', 'tour có bao gồm phương tiện di chuyển không ?', 'tour đã bao gồm phí di chuyển rồi chứ ?'], 'responses': ['Tour đã bao gồm chi phí di chuyển rồi ạ']}, {'tag': 'time_start', 'patterns': ['Thời gian bắt đầu cho chuyến đi là mấy giờ ?', 'Tour sẽ khởi hành lúc nào ?', 'Tour sẽ xuất phát lúc nào ?', 'Mấy giờ thì tour bắt đầu khởi hành'], 'responses': ['tour sẽ khởi hành trong khoảng từ 8h00-9h00 buổi sáng ngày khởi hành,quý khách vui lòng có mặt trước 8h00 để làm thủ tục và ổn định vị trí']}, {'tag': 'introduce', 'patterns': ['giới thiệu về DANANG TOURIST', 'tôi cần thông tin về DANANG TOURIST', 'DANANG TOURIST là gì', 'tôi muốn biết về DANANG TOURIST'], 'responses': ['DANANG TOURIST là công ty du lịch , chuyên về các tour du lịch của thành phố Đà Nẵng và các thành phố lân cận. Với niềm đam mê du lịch, khám phá chúng tôi đã cùng nhau xây dựng website nơi mà những nhà lữ hành có thể tìm hiểu về các tour du lịch của Đà Nẵng. Chúng tôi sẽ mong muốn sẽ cung cấp những dịch vụ tốt nhất cho quý khách <3']}, {'tag': 'hope', 'patterns': ['vì sao tôi phải chọn DANANG TOURIST', 'Cho tôi lý do vì sao tôi phải chọn bạn', 'DANANG TOURIST có gì khác so với các công ty du lịch khác ?'], 'responses': ['chúng tôi mong muốn du khách tận hưởng các dịch vụ tốt nhất qua sự trải nghiệm thực tế của chúng tôi , chúng tôi chắc chắn mang lại cho quý khách khoảng thời gian đáng nhớ với: đội ngũ hướng dẫn viên chuyên nghiệp và tận tâm , những địa điểm du lịch hấp dẫn , mức giá phải chăng.']}]}\n",
            "Tag: greeting\n",
            "Patterns:\n",
            "  - Xin chào ad\n"
          ]
        },
        {
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ASUS/nltk_data'\n    - 'c:\\\\Users\\\\ASUS\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'c:\\\\Users\\\\ASUS\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\ASUS\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ASUS\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[30], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m pattern \u001b[38;5;241m=\u001b[39m text_process(pattern)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pattern, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m pattern:  \u001b[38;5;66;03m# Đảm bảo pattern là chuỗi và không rỗng\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m     w \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Thực hiện tokenization\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     all_words\u001b[38;5;241m.\u001b[39mextend(w)\n\u001b[0;32m     16\u001b[0m     pat_tag\u001b[38;5;241m.\u001b[39mappend((w, tag))\n",
            "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
            "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
            "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
            "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:582\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    580\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    581\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 582\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ASUS/nltk_data'\n    - 'c:\\\\Users\\\\ASUS\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'c:\\\\Users\\\\ASUS\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\ASUS\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ASUS\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "all_words = []\n",
        "tags = []\n",
        "pat_tag = []\n",
        "print(intents)\n",
        "for intent in intents['intents']:\n",
        "    tag = intent['tag']  # Lấy giá trị của tag\n",
        "    patterns = intent['patterns']  # Lấy danh sách patterns tương ứng với tag\n",
        "    print(f\"Tag: {tag}\")\n",
        "    print(\"Patterns:\")\n",
        "    for pattern in patterns:\n",
        "        print(f\"  - {pattern}\")\n",
        "        pattern = text_process(pattern)\n",
        "        if isinstance(pattern, str) and pattern:  # Đảm bảo pattern là chuỗi và không rỗng\n",
        "            w = word_tokenize(pattern)  # Thực hiện tokenization\n",
        "            all_words.extend(w)\n",
        "            pat_tag.append((w, tag))\n",
        "        else:\n",
        "            print(f\"Error: pattern is invalid (type: {type(pattern)}, value: {pattern})\")\n",
        "# for intent in intents['intents']:\n",
        "#     tag = intent['tag']\n",
        "#     tags.append(tag)\n",
        "#     for pattern in intent['patterns']:\n",
        "#         pattern = text_process(pattern)\n",
        "#         w = word_tokenize(pattern)\n",
        "#         all_words.extend(w)\n",
        "#         pat_tag.append((w,tag))\n",
        "\n",
        "# for intent in intents.get('intents', []):\n",
        "#     tag = intent.get('tag', '')\n",
        "#     tags.append(tag)\n",
        "#     for pattern in intent.get('patterns', []):\n",
        "#         pattern = text_process(pattern)\n",
        "#         w = word_tokenize(pattern)  # Thực hiện tokenization\n",
        "#         all_words.extend(w)\n",
        "#         pat_tag.append((w, tag))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ASUS/nltk_data'\n    - 'c:\\\\Users\\\\ASUS\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'c:\\\\Users\\\\ASUS\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\ASUS\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ASUS\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[32], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Kiểm tra nếu pattern là chuỗi hợp lệ\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pattern, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m pattern:\n\u001b[1;32m---> 21\u001b[0m     w \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Thực hiện tokenization\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     all_words\u001b[38;5;241m.\u001b[39mextend(w)  \u001b[38;5;66;03m# Thêm các từ vào danh sách all_words\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     pat_tag\u001b[38;5;241m.\u001b[39mappend((w, tag))  \u001b[38;5;66;03m# Ghép các từ với tag\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
            "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
            "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
            "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:582\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    580\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    581\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 582\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
            "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ASUS/nltk_data'\n    - 'c:\\\\Users\\\\ASUS\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data'\n    - 'c:\\\\Users\\\\ASUS\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\ASUS\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ASUS\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "\n",
        "all_words = []\n",
        "tags = []\n",
        "pat_tag = []\n",
        "\n",
        "\n",
        "# Vòng lặp qua các intents\n",
        "for intent in intents['intents']:\n",
        "    tag = intent['tag']  # Lấy tag hiện tại\n",
        "    tags.append(tag)\n",
        "    for pattern in intent['patterns']:\n",
        "        # Giả sử bạn có một hàm text_process để xử lý văn bản trước khi token hóa\n",
        "        pattern = pattern.strip()  # Loại bỏ khoảng trắng đầu cuối (nếu cần)\n",
        "        \n",
        "        # Kiểm tra nếu pattern là chuỗi hợp lệ\n",
        "        if isinstance(pattern, str) and pattern:\n",
        "            w = word_tokenize(pattern)  # Thực hiện tokenization\n",
        "            all_words.extend(w)  # Thêm các từ vào danh sách all_words\n",
        "            pat_tag.append((w, tag))  # Ghép các từ với tag\n",
        "        else:\n",
        "            print(f\"Error: pattern is invalid (type: {type(pattern)}, value: {pattern})\")\n",
        "\n",
        "# In kết quả ra để kiểm tra\n",
        "print(\"All words:\", all_words)\n",
        "print(\"Patterns with tags:\", pat_tag)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "QuSc36Q2h6EN"
      },
      "outputs": [],
      "source": [
        "ignore_char = ['?','!','.',',',\"ad\", \"ạ\", \"ơi\", \"cho\", \"hỏi\", \"về\", \"làm\", \"thế\", \"nào\", \"có\",\n",
        "    \"của\", \"cho\", \"ở\", \"và\", \"có\", \"không\", \"này\", \"khi\", \"để\", \"đi\",\n",
        "    \"em\", \"mình\", \"bạn\", \"có\", \"gì\", \"nhé\", \"được\", \"ra\", \"sao\", \"mấy\",\n",
        "    \"đó\", \"người\", \"như\", \"từ\", \"là\", \"cách\", \"nhưng\", \"theo\", \"hay\",\n",
        "    \"với\", \"nên\", \"sẽ\", \"những\", \"làm\", \"nói\", \"được\", \"nếu\", \"đã\"]\n",
        "bag_word = [word for word in all_words if word not in ignore_char]\n",
        "bag_word = sorted(set(bag_word))\n",
        "tags = sorted(set(tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "vCWcZWbfjrIq"
      },
      "outputs": [],
      "source": [
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "for (pattern_sen,tag) in pat_tag:\n",
        "    vec = sen_to_vec(pattern_sen,bag_word)\n",
        "    X_train.append(vec)\n",
        "\n",
        "    label = tags.index(tag)\n",
        "    y_train.append([label])\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "y_train = to_categorical(y_train, len(tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLsgPIDBnoZl",
        "outputId": "24a2b6a6-4890-42b7-95bb-3e41cb82f5b8"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "index 0 is out of bounds for axis 0 with size 0",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[35], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[1;32m----> 3\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;28mlen\u001b[39m(bag_word), input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m),), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dropout(\u001b[38;5;241m0.5\u001b[39m))\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m64\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
            "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(len(bag_word), input_shape=(len(X_train[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(tags), activation='softmax'))\n",
        "\n",
        "initial_learning_rate = 0.01\n",
        "\n",
        "sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=200, batch_size=5, verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1roXOVMalCA",
        "outputId": "07b90b78-5973-46f3-f26c-96a5c68fee62"
      },
      "outputs": [
        {
          "ename": "UnicodeDecodeError",
          "evalue": "'charmap' codec can't decode byte 0x9d in position 1884: character maps to <undefined>",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[36], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_vector.json\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m----> 2\u001b[0m     states \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m all_words_state \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m state_tags \u001b[38;5;241m=\u001b[39m []\n",
            "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[0;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[0;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
            "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m codecs\u001b[38;5;241m.\u001b[39mcharmap_decode(\u001b[38;5;28minput\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,decoding_table)[\u001b[38;5;241m0\u001b[39m]\n",
            "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 1884: character maps to <undefined>"
          ]
        }
      ],
      "source": [
        "with open('state_vector.json','r') as file:\n",
        "    states = json.load(file)\n",
        "\n",
        "\n",
        "all_words_state = []\n",
        "state_tags = []\n",
        "pat_tag = []\n",
        "for state in states['state_vector']:\n",
        "    state_tag = state['tag']\n",
        "    state_tags.append(state_tag)\n",
        "    for pattern in state['patterns']:\n",
        "        pattern = text_process(pattern)\n",
        "        w = word_tokenize(pattern)\n",
        "        all_words_state.extend(w)\n",
        "        pat_tag.append((w,state_tag))\n",
        "\n",
        "\n",
        "ignore_char = ['?','!','.',',','học']\n",
        "bag_word_state = [word for word in all_words_state if word not in ignore_char]\n",
        "bag_word_state = sorted(set(bag_word_state))\n",
        "state_tags = sorted(set(state_tags))\n",
        "\n",
        "\n",
        "X_train_state = []\n",
        "y_train_state = []\n",
        "for (pattern_sen,tag) in pat_tag:\n",
        "    vec = sen_to_vec(pattern_sen,bag_word_state)\n",
        "    X_train_state.append(vec)\n",
        "    label = state_tags.index(tag)\n",
        "    y_train_state.append([label])\n",
        "X_train_state = np.array(X_train_state)\n",
        "y_train_state = np.array(y_train_state)\n",
        "y_train_state = to_categorical(y_train_state, len(state_tags))\n",
        "\n",
        "\n",
        "model_state= Sequential()\n",
        "model_state.add(Dense(len(bag_word_state), input_shape=(len(X_train_state[0]),), activation='relu'))\n",
        "model_state.add(Dropout(0.5))\n",
        "model_state.add(Dense(16, activation='relu'))\n",
        "model_state.add(Dropout(0.5))\n",
        "model_state.add(Dense(len(state_tags), activation='softmax'))\n",
        "\n",
        "sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
        "\n",
        "model_state.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "model_state.fit(X_train_state, y_train_state, epochs=200, batch_size=5, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "gueqIutEhJwy"
      },
      "outputs": [],
      "source": [
        "def get_pred_intent(chat,bag_word):\n",
        "    chat = text_process(chat)\n",
        "    w = np.array(word_tokenize(chat))\n",
        "    vec = sen_to_vec(w,bag_word).reshape(1,-1)\n",
        "    pred = model.predict(vec)\n",
        "    prob = pred[0, pred.argmax(axis=-1)[0]]\n",
        "    tag = tags[pred.argmax(axis=-1)[0]]\n",
        "    return prob,tag\n",
        "def get_pred_state(chat,bag_word):\n",
        "    chat = text_process(chat)\n",
        "    w = np.array(word_tokenize(chat))\n",
        "    vec = sen_to_vec(w,bag_word).reshape(1,-1)\n",
        "    pred = model_state.predict(vec)\n",
        "    prob_state = pred[0, pred.argmax(axis=-1)[0]]\n",
        "    state = state_tags[pred.argmax(axis=-1)[0]]\n",
        "    return prob_state, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "_6fYAdF3lb1Y"
      },
      "outputs": [
        {
          "ename": "UnicodeDecodeError",
          "evalue": "'charmap' codec can't decode byte 0x90 in position 748: character maps to <undefined>",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[38], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatabase.json\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m----> 2\u001b[0m     db \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(fp\u001b[38;5;241m.\u001b[39mread(),\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[0;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[0;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
            "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m codecs\u001b[38;5;241m.\u001b[39mcharmap_decode(\u001b[38;5;28minput\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,decoding_table)[\u001b[38;5;241m0\u001b[39m]\n",
            "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x90 in position 748: character maps to <undefined>"
          ]
        }
      ],
      "source": [
        "with open('database.json','r') as file:\n",
        "    db = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "nzzOdSFtccYi",
        "outputId": "a6d7ed5a-38a1-4a46-be07-cc84c179c391"
      },
      "outputs": [],
      "source": [
        "db['tuition']['Bigdata & Machinelearning']\n",
        "db['std_point']['Bigdata & Machinelearning']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lk_ajORxe9uE",
        "outputId": "bd568eed-1997-48f7-8e74-cf424a00cc83"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (2479776384.py, line 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  Cell \u001b[1;32mIn[39], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(\"-------Bắt đầu-------\\n\") Xin chào, mình là chatbot ảo có thể cung cấp cho bạn các thông tin \\n liên quan đến tuyển sinh đại học như là: học phí, điểm chuẩn,... tại trường đại học Duy Tân\"\u001b[0m\n\u001b[1;37m                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "bot_name = 'Bot'\n",
        "print(\"-------Bắt đầu-------\\n\") Xin chào, mình là chatbot ảo có thể cung cấp cho bạn các thông tin \\n liên quan đến tuyển sinh đại học như là: học phí, điểm chuẩn,... tại trường đại học Duy Tân\"\n",
        "print(f\"{bot_name}:)\n",
        "slots =[]\n",
        "while 1:\n",
        "\n",
        "    chat = text_process(input(\"User: \"))\n",
        "    if chat == \"quit\":\n",
        "        print(\"Cảm ơn bạn đã sử dụng Chatbot!\")\n",
        "        break\n",
        "\n",
        "    prob_intent, tag = get_pred_intent(chat,bag_word)\n",
        "    print(\"1.\",prob_intent,tag)\n",
        "    prob_state, state = get_pred_state(chat,bag_word_state)\n",
        "    print(\"2.\",prob_state,state)\n",
        "    if prob_state > 0.85:\n",
        "        slots.clear()\n",
        "        slots.insert(1,state)\n",
        "    if tag != \"unknown\" and prob_intent > 0.85:\n",
        "        slots.insert(0,tag)\n",
        "    if slots[0] in ['tuition','std_point'] or slots[0] in state_tags:\n",
        "        while (len(slots)!=2):\n",
        "            if slots[0] in state_tags:#thieu intent\n",
        "                while 1:\n",
        "                    print(f\"{bot_name}: Bạn cần hỏi thông tin nào của ngành {slots[0]}?\")\n",
        "                    chat = text_process(input(\"User: \"))\n",
        "                    if chat == \"quit\":\n",
        "                        print(\"Cảm ơn bạn đã sử dụng Chatbot!\")\n",
        "                        exit(0)\n",
        "                    prob_intent, tag = get_pred_intent(chat,bag_word)\n",
        "                    print(\"1.\",prob_intent, tag)\n",
        "                    if prob_intent > 0.85:\n",
        "                        slots.insert(0,tag)\n",
        "                        break\n",
        "            else:\n",
        "                while 1: #thieu state doi tuong\n",
        "                    print(f\"{bot_name}: Bạn cần hỏi thông tin này của ngành nào?\")\n",
        "                    chat = text_process(input(\"User: \"))\n",
        "                    if chat == \"quit\":\n",
        "                        print(\"Cảm ơn bạn đã sử dụng Chatbot!\")\n",
        "                        exit(0)\n",
        "                    prob_state, state = get_pred_state(chat,bag_word_state)\n",
        "                    print(\"2.\",prob_state, state)\n",
        "                    if prob_state > 0.85:\n",
        "                        slots.insert(1,state)\n",
        "                        break\n",
        "        if slots[0] in ['tuition','std_point']:\n",
        "            for intent in intents[\"intents\"]:\n",
        "                if tag == intent['tag']:\n",
        "                    res = intent['responses'][0].format(slots[1],db[slots[0]][slots[1]])\n",
        "                    print(f\"{bot_name}: {res}\")\n",
        "                    break;\n",
        "        else:\n",
        "            for intent in intents[\"intents\"]:\n",
        "                if tag == intent['tag']:\n",
        "                    print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
        "                    break;\n",
        "        slots.pop(0)\n",
        "    else:\n",
        "        for intent in intents[\"intents\"]:\n",
        "            if tag == intent['tag']:\n",
        "                print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
        "                break;\n",
        "        slots.clear()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
